## Where the vocab(s) will be written
save_data: None
# Prevent overwriting existing files in the folder
overwrite: True

# filter long examples
src_seq_length: 500
tgt_seq_length: 500
src_seq_length_trunc: 500
tgt_seq_length_trunc: 500

# common vocabulary for CoCoNut and target
share_vocab: True
dynamic_dict: True
# Corpus opts:
data:
    train:
        path_src: /root/zwk/NPR_DATA0306/Processed_Codit/train/prev.token
        path_tgt: /root/zwk/NPR_DATA0306/Processed_Codit/train/next.token
        transforms: []
        weight: 1
    valid:
        path_src: /root/zwk/NPR_DATA0306/Processed_Codit/val/prev.token
        path_tgt: /root/zwk/NPR_DATA0306/Processed_Codit/val/next.token
        transforms: []

src_vocab_size: 200000
tgt_vocab_size: 200000
src_vocab: /root/zwk/NPR_DATA0306/Processed_Codit/codit_token.src.vocab
tgt_vocab: /root/zwk/NPR_DATA0306/Processed_Codit/codit_token.tgt.vocab


#log_file

copy_attn: true
copy_loss_by_seqlength: true
reuse_copy_attn: true
global_attention: general
word_vec_size: 512
rnn_size: 512
bridge: true
layers: 2
encoder_type: brnn
early_stopping: 3
early_stopping_criteria: accuracy
train_steps: 500000
valid_steps: 5000
save_checkpoint_steps: 5000
max_grad_norm: 2
dropout: 0.1
batch_size: 32
valid_batch_size: 8
optim: adagrad
learning_rate: 0.15
adagrad_accumulator_init: 0.1

seed: 912
world_size: 1
gpu_ranks: 0

log_file: /root/zwk/NPR_DATA0306/Processed_Codit/save/token_template/token_template.log
save_model: /root/zwk/NPR_DATA0306/Processed_Codit/save/token_template/token_template
tensorboard: True
tensorboard_log_dir: /root/zwk/NPR_DATA0306/Processed_Codit/save/token_template/token_template.tensorlog

