## Where the vocab(s) will be written
save_data: /root/zwk/DDPR_DATA/data/M1000_Tufano
# Prevent overwriting existing files in the folder
overwrite: True

# filter long examples
src_seq_length: 1010
tgt_seq_length: 1010
src_seq_length_trunc: 1010
tgt_seq_length_trunc: 1010

# common vocabulary for CoCoNut and target
share_vocab: True
dynamic_dict: True
# Corpus opts:
data:
    train:
        path_src: /root/zwk/DDPR_DATA/data/M1000_Tufano/trn.buggy
        path_tgt: /root/zwk/DDPR_DATA/data/M1000_Tufano/trn.fix
        transforms: []
        weight: 1
    valid:
        path_src: /root/zwk/DDPR_DATA/data/M1000_Tufano/val.buggy
        path_tgt: /root/zwk/DDPR_DATA/data/M1000_Tufano/val.fix
        transforms: []

src_vocab_size: 40000
tgt_vocab_size: 40000

src_vocab: /root/zwk/DDPR_DATA/data/M1000_Tufano/trn.buggy/idioms_2w.vocab
tgt_vocab: /root/zwk/DDPR_DATA/data/M1000_Tufano/trn.buggy/idioms_2w.vocab


decoder_type: transformer
encoder_type: transformer
word_vec_size: 512
rnn_size: 512
layers: 6
transformer_ff: 2048
heads: 8
copy_attn: true
copy_loss_by_seqlength: true
reuse_copy_attn: true
gpu_ranks: 0
global_attention: general
bridge: true

accum_count: 2
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
decay_method: noam
learning_rate: 2.0
max_grad_norm: 0.0

batch_size: 2048
batch_type: tokens
normalization: tokens
dropout: 0.1
label_smoothing: 0.1

max_generator_batches: 2

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'
train_steps: 1000000
valid_steps: 5000
save_checkpoint_steps: 5000
valid_batch_size: 512
adagrad_accumulator_init: 0.1

seed: 912
world_size: 1

#model save and log
log_file: /root/zwk/DDPR_DATA/data/M1000_Tufano/20890_CATufano_transformerwv512.log
save_model: /root/zwk/DDPR_DATA/data/M1000_Tufano/20890_CATufano_transformerwv512
tensorboard: True
tensorboard_log_dir: /root/zwk/DDPR_DATA/data/M1000_Tufano/20890_CATufano_transformerwv512.tensorlog
